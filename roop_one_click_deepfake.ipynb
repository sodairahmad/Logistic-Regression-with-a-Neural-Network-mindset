{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sodairahmad/Logistic-Regression-with-a-Neural-Network-mindset/blob/main/roop_one_click_deepfake.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D3L8Lkyj-5gl"
      },
      "source": [
        "#Clone roop repo and install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aHr4Fo-7IRy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2d463ca1-c628-48c9-cc6e-dde2c5df6c8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'roop'...\n",
            "remote: Enumerating objects: 1528, done.\u001b[K\n",
            "remote: Counting objects: 100% (563/563), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 1528 (delta 496), reused 463 (delta 442), pack-reused 965\u001b[K\n",
            "Receiving objects: 100% (1528/1528), 97.42 MiB | 38.59 MiB/s, done.\n",
            "Resolving deltas: 100% (916/916), done.\n",
            "/content/roop\n",
            "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu118\n",
            "Ignoring tkinterdnd2-universal: markers 'sys_platform == \"darwin\" and platform_machine == \"arm64\"' don't match your environment\n",
            "Ignoring onnxruntime: markers 'python_version != \"3.9\" and sys_platform == \"darwin\" and platform_machine != \"arm64\"' don't match your environment\n",
            "Ignoring onnxruntime-coreml: markers 'python_version == \"3.9\" and sys_platform == \"darwin\" and platform_machine != \"arm64\"' don't match your environment\n",
            "Ignoring onnxruntime-silicon: markers 'sys_platform == \"darwin\" and platform_machine == \"arm64\"' don't match your environment\n",
            "Collecting numpy==1.24.3 (from -r requirements.txt (line 3))\n",
            "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opencv-python==4.8.0.74 (from -r requirements.txt (line 4))\n",
            "  Downloading opencv_python-4.8.0.74-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnx==1.14.0 (from -r requirements.txt (line 5))\n",
            "  Downloading onnx-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting insightface==0.7.3 (from -r requirements.txt (line 6))\n",
            "  Downloading insightface-0.7.3.tar.gz (439 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m439.5/439.5 kB\u001b[0m \u001b[31m46.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: psutil==5.9.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (5.9.5)\n",
            "Collecting tk==0.1.0 (from -r requirements.txt (line 8))\n",
            "  Downloading tk-0.1.0-py3-none-any.whl (3.9 kB)\n",
            "Collecting customtkinter==5.2.0 (from -r requirements.txt (line 9))\n",
            "  Downloading customtkinter-5.2.0-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.6/295.6 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tkinterdnd2==0.3.0 (from -r requirements.txt (line 10))\n",
            "  Downloading tkinterdnd2-0.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.7/386.7 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow==10.0.0 (from -r requirements.txt (line 12))\n",
            "  Downloading Pillow-10.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m54.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting onnxruntime-gpu==1.15.1 (from -r requirements.txt (line 16))\n",
            "  Downloading onnxruntime_gpu-1.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow==2.13.0 (from -r requirements.txt (line 17))\n",
            "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting opennsfw2==0.10.2 (from -r requirements.txt (line 18))\n",
            "  Downloading opennsfw2-0.10.2-py3-none-any.whl (12 kB)\n",
            "Collecting protobuf==4.23.4 (from -r requirements.txt (line 19))\n",
            "  Downloading protobuf-4.23.4-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m304.5/304.5 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm==4.65.0 (from -r requirements.txt (line 20))\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gfpgan==1.3.8 (from -r requirements.txt (line 21))\n",
            "  Downloading gfpgan-1.3.8-py3-none-any.whl (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.2/52.2 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.10/dist-packages (from onnx==1.14.0->-r requirements.txt (line 5)) (4.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from insightface==0.7.3->-r requirements.txt (line 6)) (2.31.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from insightface==0.7.3->-r requirements.txt (line 6)) (3.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from insightface==0.7.3->-r requirements.txt (line 6)) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from insightface==0.7.3->-r requirements.txt (line 6)) (1.2.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from insightface==0.7.3->-r requirements.txt (line 6)) (0.19.3)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.10/dist-packages (from insightface==0.7.3->-r requirements.txt (line 6)) (1.10)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from insightface==0.7.3->-r requirements.txt (line 6)) (0.29.36)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (from insightface==0.7.3->-r requirements.txt (line 6)) (1.3.1)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.10/dist-packages (from insightface==0.7.3->-r requirements.txt (line 6)) (3.8.0)\n",
            "Collecting darkdetect (from customtkinter==5.2.0->-r requirements.txt (line 9))\n",
            "  Downloading darkdetect-0.8.0-py3-none-any.whl (9.0 kB)\n",
            "Collecting coloredlogs (from onnxruntime-gpu==1.15.1->-r requirements.txt (line 16))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu==1.15.1->-r requirements.txt (line 16)) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu==1.15.1->-r requirements.txt (line 16)) (23.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu==1.15.1->-r requirements.txt (line 16)) (1.12)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->-r requirements.txt (line 17)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->-r requirements.txt (line 17)) (1.6.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->-r requirements.txt (line 17)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->-r requirements.txt (line 17)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->-r requirements.txt (line 17)) (1.57.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->-r requirements.txt (line 17)) (3.9.0)\n",
            "Collecting keras<2.14,>=2.13.1 (from tensorflow==2.13.0->-r requirements.txt (line 17))\n",
            "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->-r requirements.txt (line 17)) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->-r requirements.txt (line 17)) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->-r requirements.txt (line 17)) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->-r requirements.txt (line 17)) (1.16.0)\n",
            "Collecting tensorboard<2.14,>=2.13 (from tensorflow==2.13.0->-r requirements.txt (line 17))\n",
            "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m71.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow==2.13.0->-r requirements.txt (line 17))\n",
            "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->-r requirements.txt (line 17)) (2.3.0)\n",
            "Collecting typing-extensions>=3.6.2.1 (from onnx==1.14.0->-r requirements.txt (line 5))\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->-r requirements.txt (line 17)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.13.0->-r requirements.txt (line 17)) (0.33.0)\n",
            "Requirement already satisfied: gdown>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from opennsfw2==0.10.2->-r requirements.txt (line 18)) (4.6.6)\n",
            "Collecting basicsr>=1.4.2 (from gfpgan==1.3.8->-r requirements.txt (line 21))\n",
            "  Downloading basicsr-1.4.2.tar.gz (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting facexlib>=0.2.5 (from gfpgan==1.3.8->-r requirements.txt (line 21))\n",
            "  Downloading facexlib-0.3.0-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lmdb (from gfpgan==1.3.8->-r requirements.txt (line 21))\n",
            "  Downloading lmdb-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from gfpgan==1.3.8->-r requirements.txt (line 21)) (6.0.1)\n",
            "Collecting tb-nightly (from gfpgan==1.3.8->-r requirements.txt (line 21))\n",
            "  Downloading tb_nightly-2.15.0a20230826-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from gfpgan==1.3.8->-r requirements.txt (line 21)) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from gfpgan==1.3.8->-r requirements.txt (line 21)) (0.15.2+cu118)\n",
            "Collecting yapf (from gfpgan==1.3.8->-r requirements.txt (line 21))\n",
            "  Downloading yapf-0.40.1-py3-none-any.whl (250 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.3/250.3 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.13.0->-r requirements.txt (line 17)) (0.41.2)\n",
            "Collecting addict (from basicsr>=1.4.2->gfpgan==1.3.8->-r requirements.txt (line 21))\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from basicsr>=1.4.2->gfpgan==1.3.8->-r requirements.txt (line 21)) (0.18.3)\n",
            "Collecting filterpy (from facexlib>=0.2.5->gfpgan==1.3.8->-r requirements.txt (line 21))\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from facexlib>=0.2.5->gfpgan==1.3.8->-r requirements.txt (line 21)) (0.56.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from gdown>=4.2.0->opennsfw2==0.10.2->-r requirements.txt (line 18)) (3.12.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.2.0->opennsfw2==0.10.2->-r requirements.txt (line 18)) (4.11.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface==0.7.3->-r requirements.txt (line 6)) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface==0.7.3->-r requirements.txt (line 6)) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface==0.7.3->-r requirements.txt (line 6)) (4.42.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface==0.7.3->-r requirements.txt (line 6)) (1.4.4)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface==0.7.3->-r requirements.txt (line 6)) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->insightface==0.7.3->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface==0.7.3->-r requirements.txt (line 6)) (3.1)\n",
            "Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface==0.7.3->-r requirements.txt (line 6)) (2.31.1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface==0.7.3->-r requirements.txt (line 6)) (2023.8.12)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image->insightface==0.7.3->-r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0->-r requirements.txt (line 17)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0->-r requirements.txt (line 17)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0->-r requirements.txt (line 17)) (3.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0->-r requirements.txt (line 17)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow==2.13.0->-r requirements.txt (line 17)) (2.3.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->insightface==0.7.3->-r requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->insightface==0.7.3->-r requirements.txt (line 6)) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->insightface==0.7.3->-r requirements.txt (line 6)) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->insightface==0.7.3->-r requirements.txt (line 6)) (2023.7.22)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->gfpgan==1.3.8->-r requirements.txt (line 21)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->gfpgan==1.3.8->-r requirements.txt (line 21)) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->gfpgan==1.3.8->-r requirements.txt (line 21)) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->gfpgan==1.3.8->-r requirements.txt (line 21)) (16.0.6)\n",
            "Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface==0.7.3->-r requirements.txt (line 6)) (0.0.4)\n",
            "Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations->insightface==0.7.3->-r requirements.txt (line 6)) (4.8.0.76)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu==1.15.1->-r requirements.txt (line 16))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable->insightface==0.7.3->-r requirements.txt (line 6)) (0.2.6)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->insightface==0.7.3->-r requirements.txt (line 6)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->insightface==0.7.3->-r requirements.txt (line 6)) (3.2.0)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime-gpu==1.15.1->-r requirements.txt (line 16)) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=6.6.0 in /usr/local/lib/python3.10/dist-packages (from yapf->gfpgan==1.3.8->-r requirements.txt (line 21)) (6.8.0)\n",
            "Requirement already satisfied: platformdirs>=3.5.1 in /usr/local/lib/python3.10/dist-packages (from yapf->gfpgan==1.3.8->-r requirements.txt (line 21)) (3.10.0)\n",
            "Requirement already satisfied: tomli>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from yapf->gfpgan==1.3.8->-r requirements.txt (line 21)) (2.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0->-r requirements.txt (line 17)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0->-r requirements.txt (line 17)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0->-r requirements.txt (line 17)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0->-r requirements.txt (line 17)) (1.3.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=6.6.0->yapf->gfpgan==1.3.8->-r requirements.txt (line 21)) (3.16.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow==2.13.0->-r requirements.txt (line 17)) (2.1.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.2.0->opennsfw2==0.10.2->-r requirements.txt (line 18)) (2.4.1)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->facexlib>=0.2.5->gfpgan==1.3.8->-r requirements.txt (line 21)) (0.39.1)\n",
            "INFO: pip is looking at multiple versions of numba to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting numba (from facexlib>=0.2.5->gfpgan==1.3.8->-r requirements.txt (line 21))\n",
            "  Downloading numba-0.57.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m98.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llvmlite<0.41,>=0.40.0dev0 (from numba->facexlib>=0.2.5->gfpgan==1.3.8->-r requirements.txt (line 21))\n",
            "  Downloading llvmlite-0.40.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (42.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests->insightface==0.7.3->-r requirements.txt (line 6)) (1.7.1)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow==2.13.0->-r requirements.txt (line 17)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow==2.13.0->-r requirements.txt (line 17)) (3.2.2)\n",
            "Building wheels for collected packages: insightface, basicsr, filterpy\n",
            "  Building wheel for insightface (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for insightface: filename=insightface-0.7.3-cp310-cp310-linux_x86_64.whl size=1053162 sha256=b35d245140b76ae1519091cb60e469dff0889163b9f8d1ce6f5676e997326f8b\n",
            "  Stored in directory: /root/.cache/pip/wheels/e3/d0/80/e3773fb8b6d1cca87ea1d33d9b1f20a223a6493c896da249b5\n",
            "  Building wheel for basicsr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for basicsr: filename=basicsr-1.4.2-py3-none-any.whl size=214818 sha256=96896ae7ced5a6e598f3cef6bd79601ceb8f9343a8b7b854848bb20a888123b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/38/83/99/2d8437cc652a01af27df5ff037a4075e95b52d67705c5f30ca\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110459 sha256=9ae6a0284e98daae4365ccf7f3d89edae8934ada615c2785f719a5cf5d1c641c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0f/0c/ea/218f266af4ad626897562199fbbcba521b8497303200186102\n",
            "Successfully built insightface basicsr filterpy\n",
            "Installing collected packages: tk, lmdb, addict, typing-extensions, tqdm, tkinterdnd2, tensorflow-estimator, protobuf, pillow, numpy, llvmlite, keras, humanfriendly, darkdetect, yapf, opencv-python, onnx, numba, customtkinter, coloredlogs, onnxruntime-gpu, tensorboard, tb-nightly, filterpy, tensorflow, opennsfw2, insightface, facexlib, basicsr, gfpgan\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.66.1\n",
            "    Uninstalling tqdm-4.66.1:\n",
            "      Successfully uninstalled tqdm-4.66.1\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.20.3\n",
            "    Uninstalling protobuf-3.20.3:\n",
            "      Successfully uninstalled protobuf-3.20.3\n",
            "  Attempting uninstall: pillow\n",
            "    Found existing installation: Pillow 9.4.0\n",
            "    Uninstalling Pillow-9.4.0:\n",
            "      Successfully uninstalled Pillow-9.4.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.23.5\n",
            "    Uninstalling numpy-1.23.5:\n",
            "      Successfully uninstalled numpy-1.23.5\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.39.1\n",
            "    Uninstalling llvmlite-0.39.1:\n",
            "      Successfully uninstalled llvmlite-0.39.1\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.8.0.76\n",
            "    Uninstalling opencv-python-4.8.0.76:\n",
            "      Successfully uninstalled opencv-python-4.8.0.76\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.56.4\n",
            "    Uninstalling numba-0.56.4:\n",
            "      Successfully uninstalled numba-0.56.4\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.3\n",
            "    Uninstalling tensorboard-2.12.3:\n",
            "      Successfully uninstalled tensorboard-2.12.3\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydantic 2.2.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.6.1 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "tensorflow-metadata 1.14.0 requires protobuf<4.21,>=3.20.3, but you have protobuf 4.23.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed addict-2.4.0 basicsr-1.4.2 coloredlogs-15.0.1 customtkinter-5.2.0 darkdetect-0.8.0 facexlib-0.3.0 filterpy-1.4.5 gfpgan-1.3.8 humanfriendly-10.0 insightface-0.7.3 keras-2.13.1 llvmlite-0.40.1 lmdb-1.4.1 numba-0.57.1 numpy-1.24.3 onnx-1.14.0 onnxruntime-gpu-1.15.1 opencv-python-4.8.0.74 opennsfw2-0.10.2 pillow-10.0.0 protobuf-4.23.4 tb-nightly-2.15.0a20230826 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tk-0.1.0 tkinterdnd2-0.3.0 tqdm-4.65.0 typing-extensions-4.5.0 yapf-0.40.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!git clone https://github.com/s0md3v/roop.git\n",
        "%cd roop\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl9zIFZYGWTQ"
      },
      "source": [
        "#download model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UQH2exUYGTbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc0c9d1-fdb1-4baa-8159-3861a53fad36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-08-27 05:12:18--  https://huggingface.co/ezioruan/inswapper_128.onnx/resolve/main/inswapper_128.onnx\n",
            "Resolving huggingface.co (huggingface.co)... 18.172.134.24, 18.172.134.124, 18.172.134.4, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.172.134.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/40/3c/403ce23d3f5c02a28fcbe749205d5c8245b2812e6c948bae7abac24495779bc7/e4a3f08c753cb72d04e10aa0f7dbe3deebbf39567d4ead6dce08e98aa49e16af?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27inswapper_128.onnx%3B+filename%3D%22inswapper_128.onnx%22%3B&Expires=1693369229&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MzM2OTIyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80MC8zYy80MDNjZTIzZDNmNWMwMmEyOGZjYmU3NDkyMDVkNWM4MjQ1YjI4MTJlNmM5NDhiYWU3YWJhYzI0NDk1Nzc5YmM3L2U0YTNmMDhjNzUzY2I3MmQwNGUxMGFhMGY3ZGJlM2RlZWJiZjM5NTY3ZDRlYWQ2ZGNlMDhlOThhYTQ5ZTE2YWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=jyz2qgmqzaMAkQHyWQJxdDf4AfOIS9AP2McrX8jrKxF1u7yeRSfZyFs5OsN7uDCAaLsE6u2M%7Egu85zJzQ6XG%7ETwffB1GVOnqmLoMwFNqG-nEW3lBmLydKgU0rmRqlAIZPgQ0P1mAmp3Qjqk7tfT8YYo8aXD5nHbcKJwliVo-Blfj8ctD7nhkijnoFqcM8s9AcymyCXjY0oq%7EBGD-MAKiTPyxdUD3bX2Es0jypVPBDBlDz3-A0hjaHAihnI-90ybowC4jFbqtlisFUijT4KtLinPjIL7RKmrS7kMeZi0vquD4TCBeNvtQkd1ekRp2wzJngf52EJcQCkr3zT5jMGRVOQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2023-08-27 05:12:18--  https://cdn-lfs.huggingface.co/repos/40/3c/403ce23d3f5c02a28fcbe749205d5c8245b2812e6c948bae7abac24495779bc7/e4a3f08c753cb72d04e10aa0f7dbe3deebbf39567d4ead6dce08e98aa49e16af?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27inswapper_128.onnx%3B+filename%3D%22inswapper_128.onnx%22%3B&Expires=1693369229&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTY5MzM2OTIyOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy80MC8zYy80MDNjZTIzZDNmNWMwMmEyOGZjYmU3NDkyMDVkNWM4MjQ1YjI4MTJlNmM5NDhiYWU3YWJhYzI0NDk1Nzc5YmM3L2U0YTNmMDhjNzUzY2I3MmQwNGUxMGFhMGY3ZGJlM2RlZWJiZjM5NTY3ZDRlYWQ2ZGNlMDhlOThhYTQ5ZTE2YWY%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=jyz2qgmqzaMAkQHyWQJxdDf4AfOIS9AP2McrX8jrKxF1u7yeRSfZyFs5OsN7uDCAaLsE6u2M%7Egu85zJzQ6XG%7ETwffB1GVOnqmLoMwFNqG-nEW3lBmLydKgU0rmRqlAIZPgQ0P1mAmp3Qjqk7tfT8YYo8aXD5nHbcKJwliVo-Blfj8ctD7nhkijnoFqcM8s9AcymyCXjY0oq%7EBGD-MAKiTPyxdUD3bX2Es0jypVPBDBlDz3-A0hjaHAihnI-90ybowC4jFbqtlisFUijT4KtLinPjIL7RKmrS7kMeZi0vquD4TCBeNvtQkd1ekRp2wzJngf52EJcQCkr3zT5jMGRVOQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.156.245.63, 108.156.245.52, 108.156.245.120, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.156.245.63|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 554253681 (529M) [binary/octet-stream]\n",
            "Saving to: ‘inswapper_128.onnx’\n",
            "\n",
            "inswapper_128.onnx  100%[===================>] 528.58M   113MB/s    in 4.2s    \n",
            "\n",
            "2023-08-27 05:12:23 (126 MB/s) - ‘inswapper_128.onnx’ saved [554253681/554253681]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/ezioruan/inswapper_128.onnx/resolve/main/inswapper_128.onnx -O inswapper_128.onnx\n",
        "!mkdir models\n",
        "!mv inswapper_128.onnx ./models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCKli1C-_EaO"
      },
      "source": [
        "#Deepfake"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDYdfL3L71i1",
        "outputId": "d02450f0-387b-486a-a44d-bc79f215f7c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 332MB [00:02, 125MB/s]               \n",
            "[ROOP.FACE-SWAPPER] Select an image for source path.\n"
          ]
        }
      ],
      "source": [
        "!python run.py --target /content/video.mp4 --output-video-quality 80 --source /content/image.jpeg -o /content/swapped.mp4 --execution-provider cuda --frame-processor face_swapper face_enhancer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dB-yigWx8zht"
      },
      "outputs": [],
      "source": [
        "#Logistic Regression with a Neural Network mindset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import copy\n",
        "import matplotlib.pyplot as plt\n",
        "import h5py\n",
        "import scipy\n",
        "from PIL import Image\n",
        "from scipy import ndimage\n",
        "from lr_utils import load_dataset\n",
        "from public_tests import *\n",
        "\n",
        "%matplotlib inline\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "id": "3PYGIK6DAP5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data (cat/non-cat)\n",
        "train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()"
      ],
      "metadata": {
        "id": "Dh-_X4sRAW2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of a picture\n",
        "index = 100\n",
        "plt.imshow(train_set_x_orig[index])\n",
        "print (\"y = \" + str(train_set_y[:, index]) + \", it's a '\" + classes[np.squeeze(train_set_y[:, index])].decode(\"utf-8\") +  \"' picture.\")"
      ],
      "metadata": {
        "id": "51-tVB-mAZlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#(≈ 3 lines of code)\n",
        "# m_train =\n",
        "# m_test =\n",
        "# num_px =\n",
        "# YOUR CODE STARTS HERE\n",
        "m_train = train_set_x_orig.shape[0]  # Number of training examples\n",
        "m_test = test_set_x_orig.shape[0]    # Number of testing examples\n",
        "num_px = train_set_x_orig.shape[1]   # Image height (or width, since it's square)\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "print (\"Number of training examples: m_train = \" + str(m_train))\n",
        "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
        "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
        "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
        "print (\"train_set_x shape: \" + str(train_set_x_orig.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x shape: \" + str(test_set_x_orig.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))"
      ],
      "metadata": {
        "id": "VWvzL_2UAcKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the training and test examples\n",
        "#(≈ 2 lines of code)\n",
        "# train_set_x_flatten = ...\n",
        "# test_set_x_flatten = ...\n",
        "# YOUR CODE STARTS HERE\n",
        "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
        "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
        "\n",
        "# YOUR CODE ENDS HERE\n",
        "\n",
        "# Check that the first 10 pixels of the second image are in the correct place\n",
        "assert np.alltrue(train_set_x_flatten[0:10, 1] == [196, 192, 190, 193, 186, 182, 188, 179, 174, 213]), \"Wrong solution. Use (X.shape[0], -1).T.\"\n",
        "assert np.alltrue(test_set_x_flatten[0:10, 1] == [115, 110, 111, 137, 129, 129, 155, 146, 145, 159]), \"Wrong solution. Use (X.shape[0], -1).T.\"\n",
        "\n",
        "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
        "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
        "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
        "print (\"test_set_y shape: \" + str(test_set_y.shape))"
      ],
      "metadata": {
        "id": "wCSZoDshAe8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set_x = train_set_x_flatten / 255.\n",
        "test_set_x = test_set_x_flatten / 255."
      ],
      "metadata": {
        "id": "dMjsHiP-Ahbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: sigmoid\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Compute the sigmoid of z\n",
        "\n",
        "    Arguments:\n",
        "    z -- A scalar or numpy array of any size.\n",
        "\n",
        "    Return:\n",
        "    s -- sigmoid(z)\n",
        "    \"\"\"\n",
        "\n",
        "    #(≈ 1 line of code)\n",
        "    # s = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    s = 1 / (1 + np.exp(-z))\n",
        "\n",
        "    # YOUR CODE ENDS HERE\n",
        "\n",
        "    return s"
      ],
      "metadata": {
        "id": "6gmL5LBqAjzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"sigmoid([0, 2]) = \" + str(sigmoid(np.array([0,2]))))\n",
        "\n",
        "sigmoid_test(sigmoid)"
      ],
      "metadata": {
        "id": "p9Lblb_WAnRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([0.5, 0, 2.0])\n",
        "output = sigmoid(x)\n",
        "print(output)"
      ],
      "metadata": {
        "id": "i230PPWMApR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: initialize_with_zeros\n",
        "\n",
        "def initialize_with_zeros(dim):\n",
        "    \"\"\"\n",
        "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
        "\n",
        "    Argument:\n",
        "    dim -- size of the w vector we want (or number of parameters in this case)\n",
        "\n",
        "    Returns:\n",
        "    w -- initialized vector of shape (dim, 1)\n",
        "    b -- initialized scalar (corresponds to the bias) of type float\n",
        "    \"\"\"\n",
        "\n",
        "    # (≈ 2 lines of code)\n",
        "    # w = ...\n",
        "    # b = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    w = np.zeros((dim, 1))\n",
        "    b = 0.0\n",
        "\n",
        "    # YOUR CODE ENDS HERE\n",
        "\n",
        "    return w, b"
      ],
      "metadata": {
        "id": "0Mp_MpsYArGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dim = 2\n",
        "w, b = initialize_with_zeros(dim)\n",
        "\n",
        "assert type(b) == float\n",
        "print (\"w = \" + str(w))\n",
        "print (\"b = \" + str(b))\n",
        "\n",
        "initialize_with_zeros_test_1(initialize_with_zeros)\n",
        "initialize_with_zeros_test_2(initialize_with_zeros)\n"
      ],
      "metadata": {
        "id": "GEUOq_BSAuJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: propagate\n",
        "\n",
        "def propagate(w, b, X, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function and its gradient for the propagation explained above\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
        "\n",
        "    Return:\n",
        "    grads -- dictionary containing the gradients of the weights and bias\n",
        "            (dw -- gradient of the loss with respect to w, thus same shape as w)\n",
        "            (db -- gradient of the loss with respect to b, thus same shape as b)\n",
        "    cost -- negative log-likelihood cost for logistic regression\n",
        "\n",
        "    Tips:\n",
        "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
        "    \"\"\"\n",
        "\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    #(≈ 2 lines of code)\n",
        "    # compute activation\n",
        "    # A = ...\n",
        "    # compute cost by using np.dot to perform multiplication.\n",
        "    # And don't use loops for the sum.\n",
        "    # cost = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "\n",
        "\n",
        "    # YOUR CODE ENDS HERE\n",
        "\n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    #(≈ 2 lines of code)\n",
        "    # dw = ...\n",
        "    # db = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    m = X.shape[1]  # Number of examples\n",
        "\n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    # Compute activation\n",
        "    A = 1 / (1 + np.exp(-(np.dot(w.T, X) + b)))  # Sigmoid function\n",
        "    # Compute cost using logistic regression loss function\n",
        "    cost = -(1 / m) * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))\n",
        "\n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    dw = (1 / m) * np.dot(X, (A - Y).T)  # Gradient of the loss w.r.t w\n",
        "    db = (1 / m) * np.sum(A - Y)         # Gradient of the loss w.r.t b\n",
        "\n",
        "    # YOUR CODE ENDS HERE\n",
        "    cost = np.squeeze(np.array(cost))\n",
        "\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return grads, cost"
      ],
      "metadata": {
        "id": "p5xO6PmqAwKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w =  np.array([[1.], [2]])\n",
        "b = 1.5\n",
        "\n",
        "# X is using 3 examples, with 2 features each\n",
        "# Each example is stacked column-wise\n",
        "X = np.array([[1., -2., -1.], [3., 0.5, -3.2]])\n",
        "Y = np.array([[1, 1, 0]])\n",
        "grads, cost = propagate(w, b, X, Y)\n",
        "\n",
        "assert type(grads[\"dw\"]) == np.ndarray\n",
        "assert grads[\"dw\"].shape == (2, 1)\n",
        "assert type(grads[\"db\"]) == np.float64\n",
        "\n",
        "\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print (\"cost = \" + str(cost))\n",
        "\n",
        "propagate_test(propagate)"
      ],
      "metadata": {
        "id": "R07rssj3AzaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: optimize\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
        "    \"\"\"\n",
        "    This function optimizes w and b by running a gradient descent algorithm\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    print_cost -- True to print the loss every 100 steps\n",
        "\n",
        "    Returns:\n",
        "    params -- dictionary containing the weights w and bias b\n",
        "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
        "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
        "\n",
        "    Tips:\n",
        "    You basically need to write down two steps and iterate through them:\n",
        "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
        "        2) Update the parameters using gradient descent rule for w and b.\n",
        "    \"\"\"\n",
        "\n",
        "    w = copy.deepcopy(w)\n",
        "    b = copy.deepcopy(b)\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        # (≈ 1 lines of code)\n",
        "        # Cost and gradient calculation\n",
        "        # grads, cost = ...\n",
        "        # YOUR CODE STARTS HERE\n",
        "        grads, cost = propagate(w, b, X, Y)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        # update rule (≈ 2 lines of code)\n",
        "        ### START CODE HERE ###\n",
        "        w = w - learning_rate * dw  # need to broadcast\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "        # YOUR CODE ENDS HERE\n",
        "\n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "\n",
        "        # update rule (≈ 2 lines of code)\n",
        "        # w = ...\n",
        "        # b = ...\n",
        "        # YOUR CODE STARTS HERE\n",
        "\n",
        "\n",
        "        # YOUR CODE ENDS HERE\n",
        "\n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "\n",
        "            # Print the cost every 100 training iterations\n",
        "            if print_cost:\n",
        "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "\n",
        "    params = {\"w\": w,\n",
        "              \"b\": b}\n",
        "\n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "\n",
        "    return params, grads, costs"
      ],
      "metadata": {
        "id": "0r6dqDUMA2R2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "params, grads, costs = optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n",
        "\n",
        "print (\"w = \" + str(params[\"w\"]))\n",
        "print (\"b = \" + str(params[\"b\"]))\n",
        "print (\"dw = \" + str(grads[\"dw\"]))\n",
        "print (\"db = \" + str(grads[\"db\"]))\n",
        "print(\"Costs = \" + str(costs))\n",
        "\n",
        "optimize_test(optimize)"
      ],
      "metadata": {
        "id": "PK7k09EWA5wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: predict\n",
        "\n",
        "def predict(w, b, X):\n",
        "    '''\n",
        "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
        "\n",
        "    Arguments:\n",
        "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
        "    b -- bias, a scalar\n",
        "    X -- data of size (num_px * num_px * 3, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
        "    '''\n",
        "\n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1, m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "\n",
        "    # Compute vector \"A\" predicting the probabilities of a cat being present in the picture\n",
        "    #(≈ 1 line of code)\n",
        "    # A = ...\n",
        "    # YOUR CODE STARTS HERE\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "\n",
        "    # YOUR CODE ENDS HERE\n",
        "\n",
        "    for i in range(A.shape[1]):\n",
        "\n",
        "        # Convert probabilities A[0,i] to actual predictions p[0,i]\n",
        "        #(≈ 4 lines of code)\n",
        "        # if A[0, i] > ____ :\n",
        "        #     Y_prediction[0,i] =\n",
        "        # else:\n",
        "        #     Y_prediction[0,i] =\n",
        "        # YOUR CODE STARTS HERE\n",
        "        Y_prediction[0, i] = 1 if A[0, i] > 0.5 else 0\n",
        "\n",
        "        # YOUR CODE ENDS HERE\n",
        "\n",
        "    return Y_prediction"
      ],
      "metadata": {
        "id": "ljFeeL2mA7Uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "w = np.array([[0.1124579], [0.23106775]])\n",
        "b = -0.3\n",
        "X = np.array([[1., -1.1, -3.2],[1.2, 2., 0.1]])\n",
        "print (\"predictions = \" + str(predict(w, b, X)))\n",
        "\n",
        "predict_test(predict)"
      ],
      "metadata": {
        "id": "MTKm-q1OA-KX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: model\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
        "    \"\"\"\n",
        "    Builds the logistic regression model by calling the function you've implemented previously\n",
        "\n",
        "    Arguments:\n",
        "    X_train -- training set represented by a numpy array of shape (num_px * num_px * 3, m_train)\n",
        "    Y_train -- training labels represented by a numpy array (vector) of shape (1, m_train)\n",
        "    X_test -- test set represented by a numpy array of shape (num_px * num_px * 3, m_test)\n",
        "    Y_test -- test labels represented by a numpy array (vector) of shape (1, m_test)\n",
        "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
        "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
        "    print_cost -- Set to True to print the cost every 100 iterations\n",
        "\n",
        "    Returns:\n",
        "    d -- dictionary containing information about the model.\n",
        "    \"\"\"\n",
        "    # (≈ 1 line of code)\n",
        "    # initialize parameters with zeros\n",
        "    # and use the \"shape\" function to get the first dimension of X_train\n",
        "    # w, b = ...\n",
        "\n",
        "    #(≈ 1 line of code)\n",
        "    # Gradient descent\n",
        "    # params, grads, costs = ...\n",
        "\n",
        "    # Retrieve parameters w and b from dictionary \"params\"\n",
        "    # w = ...\n",
        "    # b = ...\n",
        "\n",
        "    # Predict test/train set examples (≈ 2 lines of code)\n",
        "    # Y_prediction_test = ...\n",
        "    # Y_prediction_train = ...\n",
        "\n",
        "    # YOUR CODE STARTS HERE\n",
        "    # initialize parameters with zeros (≈ 1 line of code)\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "\n",
        "    # Gradient descent (≈ 1 line of code)\n",
        "    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
        "\n",
        "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "\n",
        "    # Predict test/train set examples (≈ 2 lines of code)\n",
        "    Y_prediction_test = predict(w, b, X_test)\n",
        "    Y_prediction_train = predict(w, b, X_train)\n",
        "\n",
        "    # YOUR CODE ENDS HERE\n",
        "\n",
        "    # Print train/test Errors\n",
        "    if print_cost:\n",
        "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
        "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
        "\n",
        "\n",
        "    d = {\"costs\": costs,\n",
        "         \"Y_prediction_test\": Y_prediction_test,\n",
        "         \"Y_prediction_train\" : Y_prediction_train,\n",
        "         \"w\" : w,\n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "\n",
        "    return d"
      ],
      "metadata": {
        "id": "K_UMUE77BAIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from public_tests import *\n",
        "\n",
        "model_test(model)"
      ],
      "metadata": {
        "id": "D0YUpDBZBDWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_regression_model = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=2000, learning_rate=0.005, print_cost=True)"
      ],
      "metadata": {
        "id": "4lcaPlU8BFCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of a picture that was wrongly classified.\n",
        "index = 45\n",
        "plt.imshow(test_set_x[:, index].reshape((num_px, num_px, 3)))\n",
        "print (\"y = \" + str(test_set_y[0,index]) + \", you predicted that it is a \\\"\" + classes[int(logistic_regression_model['Y_prediction_test'][0,index])].decode(\"utf-8\") +  \"\\\" picture.\")"
      ],
      "metadata": {
        "id": "WmCe6HyoBHGh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot learning curve (with costs)\n",
        "costs = np.squeeze(logistic_regression_model['costs'])\n",
        "plt.plot(costs)\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (per hundreds)')\n",
        "plt.title(\"Learning rate =\" + str(logistic_regression_model[\"learning_rate\"]))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3le87wSWBJkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rates = [0.01, 0.001, 0.0001]\n",
        "models = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print (\"Training a model with learning rate: \" + str(lr))\n",
        "    models[str(lr)] = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=1500, learning_rate=lr, print_cost=False)\n",
        "    print ('\\n' + \"-------------------------------------------------------\" + '\\n')\n",
        "\n",
        "for lr in learning_rates:\n",
        "    plt.plot(np.squeeze(models[str(lr)][\"costs\"]), label=str(models[str(lr)][\"learning_rate\"]))\n",
        "\n",
        "plt.ylabel('cost')\n",
        "plt.xlabel('iterations (hundreds)')\n",
        "\n",
        "legend = plt.legend(loc='upper center', shadow=True)\n",
        "frame = legend.get_frame()\n",
        "frame.set_facecolor('0.90')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vxHSyvwPBMEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change this to the name of your image file\n",
        "my_image = \"my_image.jpg\"\n",
        "\n",
        "# We preprocess the image to fit your algorithm.\n",
        "fname = \"images/\" + my_image\n",
        "image = np.array(Image.open(fname).resize((num_px, num_px)))\n",
        "plt.imshow(image)\n",
        "image = image / 255.\n",
        "image = image.reshape((1, num_px * num_px * 3)).T\n",
        "my_predicted_image = predict(logistic_regression_model[\"w\"], logistic_regression_model[\"b\"], image)\n",
        "\n",
        "print(\"y = \" + str(np.squeeze(my_predicted_image)) + \", your algorithm predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
      ],
      "metadata": {
        "id": "k5pBNfG5BPo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gssE01EiBSd3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}